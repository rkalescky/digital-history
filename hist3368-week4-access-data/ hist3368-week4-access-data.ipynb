{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the [homepage](https://www.gutenberg.org/) for Project Gutenberg if you are having trouble finding a specific book.\n",
    "\n",
    "Usage documentation for the Python package can be found [here](https://pypi.org/project/Gutenberg/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "text = strip_headers(load_etext(11)).strip()\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [[1342,\"Pride and Prejudice\",\"Jane Austen\"],\n",
    "         [11,\"Alice's Adventures in Wonderland\", \"Lewis Carroll\"],\n",
    "         [2701,\"Moby Dick; Or, The Whale\",\"Herman Melville\"],\n",
    "         [84,\"Frankenstein; Or, The Modern Prometheus\", \"Mary Wollenstonecraft Shelley\" ],\n",
    "         [345,\"Dracula\", \"Bram Stoker\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenDF = pd.DataFrame(books, columns=['ID','Title','Author'])\n",
    "gutenDF['FullText']=gutenDF.apply(lambda row: strip_headers(load_etext(row['ID'])).strip() , axis=1)\n",
    "gutenDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenDF[gutenDF['Title'].str.contains(\"W\")]['FullText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaData and Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on doing a lot of work with Project Gutenberg's metadata functionality, you'll need to cache their metadata first. This can take a very long time but makes it possible to query their metadata quickly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gutenberg.acquire import get_metadata_cache\n",
    "cache = get_metadata_cache()\n",
    "cache.populate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from gutenberg.query import get_etexts\n",
    "from gutenberg.query import get_metadata\n",
    "\n",
    "print(get_metadata('title', 11))  # prints frozenset([u'Moby Dick; Or, The Whale'])\n",
    "print(get_metadata('author', 11)) # prints frozenset([u'Melville, Hermann'])\n",
    "\n",
    "print(get_etexts('title', 'Moby Dick; Or, The Whale'))  # prints frozenset([2701, ...])\n",
    "print(get_etexts('author', 'Melville, Hermann'))        # prints frozenset([2701, ...])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDGAR Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the [homepage](https://www.sec.gov/edgar.shtml) for the Securities and Exchange Commission's EDGAR database. If you are having trouble finding a specific company, try their [full text search](https://www.sec.gov/edgar/search/#).\n",
    "\n",
    "Usage documentation for the Python package can be found [here](https://pypi.org/project/edgar/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the last 5 10-K reports for the Oracle Corporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgar import Company, TXTML\n",
    "company = Company(\"Oracle Corp\", \"0001341439\")\n",
    "tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "docs = Company.get_documents(tree, no_of_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the most recent 10-K filing for IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = Company(\"INTERNATIONAL BUSINESS MACHINES CORP\", \"0000051143\")\n",
    "doc = company.get_10K()\n",
    "text = TXTML.parse_full_10K(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search EDGAR for a company Cisco System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgar import Edgar\n",
    "edgar = Edgar()\n",
    "possible_companies = edgar.find_company_name(\"Cisco System\")\n",
    "possible_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [['AMAZON COM INC','0001018724'],\n",
    "            ['Alphabet Inc.','0001652044'],\n",
    "            ['MICROSOFT CORP','0000789019']\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgarDF = pd.DataFrame(companies, columns=['Company','CIK'])\n",
    "edgarDF['MostRecent_10K']=edgarDF.apply(lambda row: TXTML.parse_full_10K(Company(row['Company'],row['CIK']).get_10K()) , axis=1)\n",
    "edgarDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last 5 10Ks with Filing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edgar(ll, n):\n",
    "    filinglist = []\n",
    "    for el in ll:\n",
    "        company = Company(el[0], el[1])\n",
    "        tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "        docs = Company.get_documents(tree, no_of_documents=n, as_documents=True)\n",
    "        texts = Company.get_documents(tree, no_of_documents=n, as_documents=False)\n",
    "        if n<2:\n",
    "            docs=[docs]\n",
    "            texts=[texts]\n",
    "        for i in range(n):\n",
    "            date = docs[i].content['Filing Date']\n",
    "            text = TXTML.parse_full_10K(texts[i])\n",
    "            filinglist.append([el[0],el[1],date,text])\n",
    "    df = pd.DataFrame(filinglist, columns=['Company','CIK','10K_Filing','Filing_Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edgar(companies,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hansard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the Hansard you will need to significantly increase the memory you request for your JupyterLab session. We'd suggest upping from 6GB to 64GB. This might result in a longer wait for launching your job but will allow you to hold all of the data in dataframe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_parquet(\"/scratch/group/oit_research_data/hansard/hansard_20191119.parquet\")\n",
    "hansard.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have the option to use the Dask dataframe library instead of pandas. The tradeoff is that Dask will let you use less memory at the cost of speed and lends itself to parallelization better than pandas. When in doubt, default to using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "hansard = dd.read_parquet(\"/scratch/group/oit_research_data/hansard/hansard_20191119.parquet\")\n",
    "hansard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Hansard to study specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the speechdate column to datetime objects - forcing any errors to be set to NaN\n",
    "hansard['speechdate']=pd.to_datetime(hansard['speechdate'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only speechdates before 1900\n",
    "hansard1800s = hansard[hansard['speechdate']<dt.datetime(1900,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for speakers named Gladstone and sentences about Dublin\n",
    "hansard1800s[(hansard1800s['speaker'].str.contains('Gladstone'))&(hansard1800s['text'].str.contains('Dublin'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Congress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pulled via shell script (congress_download.sh) from [Stanford's Congressional Record](https://data.stanford.edu/congress_text) and is available on M2. If you use this data, please use the proper citation for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_congress = \"/scratch/group/oit_research_data/stanford_congress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('{}/*'.format(path_to_congress))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know how to best use this dataset, we suggest you read the codebook found [here](https://stacks.stanford.edu/file/druid:md374tz9962/codebook_v4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pulled via shell script (congress_download.sh) from [MIT's COVID-19 Open Research Dataset](https://innovation.mit.edu/cord19/) and is available on M2. If you use this data, please use the proper citation for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-08-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_covid = \"/scratch/group/oit_research_data/semantic_scholar_cord_19/\"+date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('{}/*'.format(path_to_covid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('{}/metadata.csv'.format(path_to_covid), dtype=object)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have provided a function that will take the associated files for each row in our metadata table and read those files in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_json(row,path=path_to_covid):\n",
    "    \n",
    "    file_pdf_json = '{}/{}'.format(path, row['pdf_json_files'])\n",
    "    file_pmc_json = '{}/{}'.format(path, row['pmc_json_files'])\n",
    "    \n",
    "    read = False\n",
    "    \n",
    "    try:\n",
    "        with open(file_pdf_json) as f:\n",
    "            text = json.load(f)\n",
    "            read = True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open(file_pmc_json) as f:\n",
    "                text = json.load(f)\n",
    "                read = True\n",
    "        except FileNotFoundError:\n",
    "            text = {\"body_text\":\"No Files Listed Were Found\"}\n",
    "            read = False\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDF = metadata.dropna(subset=['pdf_json_files', 'pmc_json_files'],thresh=1).sample(100) # Selects a random sample of articles from our collection that have at least 1 json file listed\n",
    "\n",
    "covidDF['Text'] = covidDF.apply(lambda row: read_text_from_json(row,path_to_covid), axis = 1) # read the text using a function to parse the associated file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulls the body text of the paper for the first row in our sample subset of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDF.iloc[0]['Text']['body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: If you want to use Reddit, you are likely going to have issues with scale due to the size of the full data set. Please reach out to us about how to best approach your data. We might be able to help you cultivate a more manageable subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Pandas with Reddit, you will need more memory than your typical node. To do this you will want to open a JupyterLab Session on a high-mem queue. For more information, look at the documentation [here](http://faculty.smu.edu/csc/documentation/slurm.html#maneframe-ii-s-slurm-partitions-queues)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reddit = pd.read_parquet(\"/scratch/group/oit_research_data/reddit/reddit.parquet\")\n",
    "reddit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to keep the data on disk instead of using a higher memory node, you will need to use a different dataframe library than pandas. Dask will let you do this but it will be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "reddit = dd.read_parquet(\"/scratch/group/oit_research_data/reddit/reddit.parquet\")\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Reddit to for a specific subreddit over a specific time. In this case, filtering for subreddits including \"climate\" before December 21st 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit=reddit[reddit['subreddit'].str.contains('climate')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the 'created_utc column' to datetime objects - forcing any errors to be set to NaN. UTC is [Unix Time](https://en.wikipedia.org/wiki/Unix_time).\n",
    "\n",
    "Then, filter for only posts before 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit['created_utc']=dd.to_datetime(subreddit['created_utc'], unit='s', errors='coerce')\n",
    "\n",
    "subreddit2012 = subreddit[subreddit['date']<dt.datetime(2012,12,21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
